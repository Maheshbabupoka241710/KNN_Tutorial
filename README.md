#  KNN Wine Quality Classification â€“ Tutorial

A student-friendly machine learning tutorial using the K-Nearest Neighbours (KNN) algorithm to classify wine quality (low / medium / high) from the WineQT dataset.

---

##  Project Overview

This project walks through an **end-to-end ML pipeline**:
from understanding KNN â†’ exploring the data â†’ preprocessing â†’ modelling â†’ tuning â†’ visualising â†’ comparing with Logistic Regression â†’ reflecting on results.

---

##  Tutorial Flow (Stage-by-Stage â€“ Crisp Summary)

### 1ï¸âƒ£ Introduction  
Explain the intuition behind KNN: â€œlook at who is closest and follow the majorityâ€, with a simple diagram showing how neighbours decide the class.

### 2ï¸âƒ£ Understanding KNN  
Break down how KNN predicts a label using k nearest neighbours, majority voting, and distance metrics (Euclidean, Manhattan, Minkowski).

### 3ï¸âƒ£ Visual Overview  
Describe how different plots (class distribution, heatmap, boxplots, PCA, confusion matrices) help us see how KNN interacts with the wine data.

### 4ï¸âƒ£ Exploratory Data Analysis (EDA)  
Check class balance, feature relationships, and key trends using plots like class distribution, correlation heatmap, and boxplots.

### 5ï¸âƒ£ Data Preprocessing  
Clean the data, create quality labels (low/medium/high), scale features using StandardScaler, and perform a stratified trainâ€“test split.

### 6ï¸âƒ£ Baseline Models  
Build a baseline KNN model and a baseline Logistic Regression model to understand initial performance and set a reference.

### 7ï¸âƒ£ Hyperparameter Tuning  
Use GridSearchCV to search over k values, distance metrics, and weight types, optimising on F1-macro to handle class imbalance.

### 8ï¸âƒ£ Model Visualisations  
Create PCA plots, F1 vs k curves, and confusion matrices for KNN and Logistic Regression to visually inspect performance and errors.

### 9ï¸âƒ£ Model Comparison  
Compare KNN and Logistic Regression, explaining why Logistic Regression slightly outperforms KNN and what this says about the data (near-linear boundaries).

### ğŸ”Ÿ Discussion  
Summarise key insights: importance of scaling, influence of alcohol and volatile acidity, impact of class imbalance, and why metrics like F1-macro matter.

### 1ï¸âƒ£1ï¸âƒ£ Limitations & Future Work  
Note dataset size and imbalance, KNNâ€™s speed on large data, and suggest improvements like SMOTE, advanced models, weighted KNN, and UMAP.

### 1ï¸âƒ£2ï¸âƒ£ Conclusion  
Wrap up by stating that KNN is simple and interpretable, Logistic Regression is slightly stronger here, and the pipeline follows good ML practice from start to finish.

---

## â–¶ï¸ How to Run

1. Clone the repo:
```bash
git clone https://github.com/your-username/KNN-Wine-Quality-Project.git
cd KNN-Wine-Quality-Project

## Project Structure

KNN-Wine-Quality-Project/
â”‚
â”œâ”€â”€ KNN_Tutorial.ipynb        # Main tutorial notebook (code + explanations)
â”œâ”€â”€ WineQT.csv                # Wine quality dataset (from Kaggle)
â”œâ”€â”€ README.md                 # This documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚
â””â”€â”€ images/                   # (Optional) Saved plots for the report
    â”œâ”€â”€ knn_workflow.png
    â”œâ”€â”€ class_distribution.png
    â”œâ”€â”€ correlation_heatmap.png
    â”œâ”€â”€ boxplot_alcohol.png
    â”œâ”€â”€ boxplot_residual_sugar.png
    â”œâ”€â”€ boxplot_volatile_acidity.png
    â”œâ”€â”€ pca_projection.png
    â”œâ”€â”€ f1_vs_k_curve.png
    â”œâ”€â”€ confusion_knn.png
    â””â”€â”€ confusion_logreg.png


